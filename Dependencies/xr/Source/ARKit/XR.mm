#import <XR.h>
#include <XRHelpers.h>

#import <UIKit/UIKit.h>
#import <ARKit/ARKit.h>
#import <ARKit/ARConfiguration.h>
#import <MetalKit/MetalKit.h>

@interface SessionDelegate : NSObject <ARSessionDelegate, MTKViewDelegate>
@end

namespace {
    typedef struct {
        vector_float2 position;
        vector_float2 uv;
        vector_float2 cameraUV;
    } XRVertex;

    struct ARAnchorComparer {
        bool operator()(const ARPlaneAnchor* lhs, const ARPlaneAnchor* rhs) const {
            return lhs.identifier < rhs.identifier;
        }
    };

    /**
     Defines the 2D positions and mapping UVs for both the camera and babylon textures. Camera UVs are generated by using the inverse transform of the camera image to match the display viewport.
     NOTE: For Metal UV space, 0,0 is in the top left, not the bottom left, so the V component of the UV is swapped from what you would see for OpenGL. See https://developer.apple.com/documentation/metal/creating_and_sampling_textures.
     */
    static XRVertex vertices[] = {
        // 2D positions, UV,        camera UV
        { { -1, -1 },   { 0, 1 },   { 0, 0} },
        { { -1, 1 },    { 0, 0 },   { 0, 0} },
        { { 1, -1 },    { 1, 1 },   { 0, 0} },
        { { 1, 1 },     { 1, 0 },   { 0, 0} },
    };

    /**
     Helper function to convert a transform into an xr::pose.
     */
    static xr::Pose TransformToPose(simd_float4x4 transform) {
        // Set orientation.
        xr::Pose pose{};
        auto orientation = simd_quaternion(transform);
        pose.Orientation = { orientation.vector.x
            , orientation.vector.y
            , orientation.vector.z
            , orientation.vector.w };
        
        //Â Set the translation.
        pose.Position = { transform.columns[3][0]
            , transform.columns[3][1]
            , transform.columns[3][2] };
        
        return pose;
    }

    /**
     Helper function to convert an xr pose into a transform.
     */
    static simd_float4x4 PoseToTransform(xr::Pose pose) {
        auto poseQuaternion = simd_quaternion(pose.Orientation.X, pose.Orientation.Y, pose.Orientation.Z, pose.Orientation.W);
        auto poseTransform = simd_matrix4x4(poseQuaternion);
        poseTransform.columns[3][0] = pose.Position.X;
        poseTransform.columns[3][1] = pose.Position.Y;
        poseTransform.columns[3][2] = pose.Position.Z;
        
        return poseTransform;
    }
}

/**
 Implementation of the ARSessionDelegate interface for more info see: https://developer.apple.com/documentation/arkit/arsessiondelegate
 */
@implementation SessionDelegate {
    std::vector<xr::System::Session::Frame::View>* activeFrameViews;
    
    NSLock* planeLock;
    std::set<ARPlaneAnchor*,ARAnchorComparer> updatedPlanes;
    std::vector<ARPlaneAnchor*> deletedPlanes;
    bool planeDetectionEnabled;
    
    CVMetalTextureCacheRef textureCache;
    CVMetalTextureRef _cameraTextureY;
    CVMetalTextureRef _cameraTextureCbCr;
    CGSize _viewportSize;

    UIInterfaceOrientation cameraUVReferenceOrientation;
    CGSize cameraUVReferenceSize;
}

/**
 Returns the camera Y texture, the caller is responsible for freeing this texture.
 */
- (id<MTLTexture>)GetCameraTextureY {
    if (_cameraTextureY != nil) {
        id<MTLTexture> mtlTexture = CVMetalTextureGetTexture(_cameraTextureY);
        return mtlTexture;
    }
    
    return nil;
}

/**
 Returns the camera CbCr texture, the caller is responsible for freeing this texture.
 */
- (id<MTLTexture>)GetCameraTextureCbCr {
    if (_cameraTextureCbCr != nil) {
        id<MTLTexture> mtlTexture = CVMetalTextureGetTexture(_cameraTextureCbCr);
        return mtlTexture;
    }
    
    return nil;
}

/**
 Returns the set of all updated planes since the last time we consumed plane updates.
 */
- (std::set<ARPlaneAnchor*, ARAnchorComparer>*) GetUpdatedPlanes {
    return &updatedPlanes;
}

/**
 Returns the vector containing all deleted planes since the last time we consumed plane updates.
 */
- (std::vector<ARPlaneAnchor*>*) GetDeletedPlanes {
    return &deletedPlanes;
}

- (void) SetPlaneDetectionEnabled:(bool)enabled {
    planeDetectionEnabled = enabled;
}

/**
 Initializes this session delgate with the given frame views and metal graphics context.
 */
- (id)init:(std::vector<xr::System::Session::Frame::View>*)activeFrameViews metalContext:(id<MTLDevice>)graphicsContext viewportSize:(CGSize)viewportSize {
    self = [super init];
    self->activeFrameViews = activeFrameViews;
    self->_viewportSize = viewportSize;

    CVReturn err = CVMetalTextureCacheCreate(kCFAllocatorDefault, nil, graphicsContext, nil, &textureCache);
    if (err) {
        throw std::runtime_error{"Unable to create Texture Cache"};
    }
    
    updatedPlanes = {};
    deletedPlanes = {};
    planeLock = [[NSLock alloc] init];
    return self;
}

- (void) LockPlanes {
    [planeLock lock];
}

- (void) UnlockPlanes {
    [planeLock unlock];
}

/**
 Returns the orientation of the app based on the current status bar orientation.
*/
- (UIInterfaceOrientation)orientation {
    auto sharedApplication = [UIApplication sharedApplication];
    auto window = sharedApplication.windows.firstObject;
    if (@available(iOS 13.0, *)) {
        return window.windowScene.interfaceOrientation;
    }
    else {
        return [[UIApplication sharedApplication] statusBarOrientation];
    }
}

/**
 Returns the viewportSize as determined by the texture size of the first active frame view.
*/
- (CGSize)viewportSize {
    auto frameSize = activeFrameViews->front().ColorTextureSize;
    return CGSizeMake(frameSize.Width, frameSize.Height);
}

/**
 Implementation of the ARSessionDelegate protocol. Called every frame during the active ARKit session.
 NOTE: If this part of the protocol is implemented, then ARKit will run its own loop and push frames to this method.
      We want to pull frames on demand since we manage our own render loop. Leaving this here commented
      out to make it easy to switch back to this mode of operation or do further experimentation as needed.
*/
//- (void)session:(ARSession *) session didUpdateFrame:(ARFrame *)frame {
//    [self session:session didUpdateFrameInternal:frame];
//}

/**
 Updates the AR Camera texture, and Camera pose. If a size change is detected also sets the UVs, and FoV values.
 */
- (void)session:(ARSession *)__unused session didUpdateFrameInternal:(ARFrame *)frame {
    @autoreleasepool{
        // Update both metal textures used by the renderer to display the camera image.
        CVMetalTextureRef newCameraTextureY = [self getCameraTexture:frame.capturedImage plane:0];
        CVMetalTextureRef newCameraTextureCbCr = [self getCameraTexture:frame.capturedImage plane:1];

        // Swap the camera textures, do this under synchronization lock to prevent null access.
        @synchronized(self) {
            [self cleanupTextures];
            _cameraTextureY = newCameraTextureY;
            _cameraTextureCbCr = newCameraTextureCbCr;
        }

        // Check if our orientation or size has changed and update camera UVs if necessary.
        if ([self checkAndUpdateCameraUVs:frame]) {
            // If our camera UVs updated, then also update the projection matrix to match the updated UVs.
            [self updateProjectionMatrix:frame.camera];
        }

        // Finally update the XR pose based on the current transform from ARKit.
        [self updateDisplayOrientedPose:(frame.camera)];
    }
}

/**
 Updates the captured texture with the current frame buffer.
*/
- (CVMetalTextureRef)getCameraTexture:(CVPixelBufferRef)pixelBuffer plane:(int)planeIndex {
    CVReturn ret = CVPixelBufferLockBaseAddress(pixelBuffer, kCVPixelBufferLock_ReadOnly);
    if (ret != kCVReturnSuccess) {
        return {};
    }

    @try {
        size_t planeWidth = CVPixelBufferGetWidthOfPlane(pixelBuffer, planeIndex);
        size_t planeHeight = CVPixelBufferGetHeightOfPlane(pixelBuffer, planeIndex);
            
        // Plane 0 is the Y plane, which is in R8Unorm format, and the second plane is the CBCR plane which is RG8Unorm format.
        auto pixelFormat = planeIndex ? MTLPixelFormatRG8Unorm : MTLPixelFormatR8Unorm;
        CVMetalTextureRef texture;
        
        // Create a texture from the corresponding plane.
        auto status = CVMetalTextureCacheCreateTextureFromImage(kCFAllocatorDefault, textureCache, pixelBuffer, nil, pixelFormat, planeWidth, planeHeight, planeIndex, &texture);
        if (status != kCVReturnSuccess) {
            return nil;
        }
        
        return texture;
    }
    @finally {
        CVPixelBufferUnlockBaseAddress(pixelBuffer, kCVPixelBufferLock_ReadOnly);
    }
}

/**
 Checks whether the camera UVs need to be updated based on the orientation and size of the view port, and updates them if necessary.
 @return True if the camera UVs were updated, false otherwise.
*/
- (Boolean)checkAndUpdateCameraUVs:(ARFrame *)frame {
    // When the orientation or viewport size changes loop over triangleVerts, apply transform to the UV to generate camera UVs.
    auto orientation = [self orientation];
    CGSize viewportSize = [self viewportSize];
    if (cameraUVReferenceOrientation != orientation || cameraUVReferenceSize.height != viewportSize.height || cameraUVReferenceSize.width != viewportSize.width) {
        // The default transform is for converting normalized image coordinates to UVs, we want the inverse as we are converting
        // UVs to normalized image coordinates.
        auto transform = CGAffineTransformInvert([frame displayTransformForOrientation:orientation viewportSize:[self viewportSize]]);
        for(size_t i = 0; i < sizeof(vertices) / sizeof(*vertices); i++) {
            CGPoint transformedPoint = CGPointApplyAffineTransform({vertices[i].uv[0], vertices[i].uv[1]}, transform);
            vertices[i].cameraUV[0] = transformedPoint.x;
            vertices[i].cameraUV[1] = transformedPoint.y;
        }
        
        // Keep track of the last known orientation and viewport size.
        cameraUVReferenceOrientation = orientation;
        cameraUVReferenceSize = viewportSize;
        return true;
    }
    
    return false;
}

/**
 Gets the projection matrix of the AR Camera, and applies it to the frameView.
*/
- (void)updateProjectionMatrix:(ARCamera*)camera {
    // Get the viewport size and the orientation of the device.
    auto& frameView = activeFrameViews->at(0);
    auto viewportSize = [self viewportSize];
    auto orientation = [self orientation];

    // Grab the projection matrix for the image based on the viewport.
    auto projectionMatrix = [camera projectionMatrixForOrientation:orientation viewportSize:viewportSize zNear:frameView.DepthNearZ zFar:frameView.DepthFarZ];
    memcpy(frameView.ProjectionMatrix.data(), projectionMatrix.columns, sizeof(float) * frameView.ProjectionMatrix.size());
}

/**
 The ARKit camera transform is always a local right hand coordinate space WRT landscape right orientation, so this function takes the transform and converts
 it into a display oriented pose see: (https://developer.apple.com/documentation/arkit/arcamera/2866108-transform)
 */
-(void)updateDisplayOrientedPose:(ARCamera*)camera {
    auto& frameView = activeFrameViews->at(0);
    UIInterfaceOrientation orientation = [self orientation];
    simd_float4x4 transform = [camera transform];
    simd_quatf displayOrientationQuat;
    
    // Create the display orientation quaternion based on the current orientation of the device.
    if (orientation == UIInterfaceOrientationLandscapeRight) {
        displayOrientationQuat = simd_quaternion(0.0f, 0.0f, 0.0f, 1.0f);
    }
    else if (orientation == UIInterfaceOrientationLandscapeLeft) {
        displayOrientationQuat = simd_quaternion((float)M_PI, simd_make_float3(0, 0, 1));
    }
    else if (orientation == UIInterfaceOrientationPortraitUpsideDown) {
        displayOrientationQuat = simd_quaternion((float)M_PI * -.5f, simd_make_float3(0, 0, 1));
    }
    else if (orientation == UIInterfaceOrientationPortrait) {
        displayOrientationQuat = simd_quaternion((float)M_PI * .5f, simd_make_float3(0, 0, 1));
    }
    
    // Convert the display orientation quaternion to a transform matrix.
    simd_float4x4 rotationMatrix = simd_matrix4x4(displayOrientationQuat);
    
    // Multiply the transform by the rotation matrix to generate the display oriented transform.
    auto displayOrientedTransform = simd_mul(transform, rotationMatrix);
    
    //Pull out the display oriented rotation.
    auto displayOrientation = simd_quaternion(displayOrientedTransform);
    
    // Set the orientation of the camera
    frameView.Space.Pose.Orientation = { displayOrientation.vector.x
        , displayOrientation.vector.y
        , displayOrientation.vector.z
        , displayOrientation.vector.w};
    
    //Â Set the translation.
    frameView.Space.Pose.Position = { displayOrientedTransform.columns[3][0]
        , displayOrientedTransform.columns[3][1]
        , displayOrientedTransform.columns[3][2] };
}

- (void)session:(ARSession *)__unused session didAddAnchors:(nonnull NSArray<__kindof ARAnchor *> *)anchors {
    if (!planeDetectionEnabled) {
        return;
    }

    [self LockPlanes];
    for (ARAnchor* newAnchor : anchors) {
        if ([newAnchor isKindOfClass:[ARPlaneAnchor class]]) {
            auto insertResult = updatedPlanes.insert((ARPlaneAnchor*)newAnchor);
            
            // We need to keep the pointer alive, so mark the anchor as retained.
            if (insertResult.second) {
                [newAnchor retain];
            }
        }
    }
    
    [self UnlockPlanes];
}

- (void)session:(ARSession *)__unused session didUpdateAnchors:(nonnull NSArray<__kindof ARAnchor *> *)anchors {
    if (!planeDetectionEnabled) {
        return;
    }

    [self LockPlanes];
    for (ARAnchor* updatedAnchor : anchors) {
        if ([updatedAnchor isKindOfClass:[ARPlaneAnchor class]]) {
            auto insertResult = updatedPlanes.insert((ARPlaneAnchor*)updatedAnchor);
            
            // We need to keep the pointer alive, so mark the anchor as retained.
            if (insertResult.second) {
                [updatedAnchor retain];
            }
        }
    }
    
    [self UnlockPlanes];
}

- (void)session:(ARSession *)__unused session didRemoveAnchors:(nonnull NSArray<__kindof ARAnchor *> *)anchors {
    if (!planeDetectionEnabled) {
        return;
    }

    [self LockPlanes];
    for (ARAnchor* removedAnchor : anchors) {
        if ([removedAnchor isKindOfClass:[ARPlaneAnchor class]]) {
            deletedPlanes.push_back((ARPlaneAnchor*)removedAnchor);
            [removedAnchor retain];
        }
    }
    
    [self UnlockPlanes];
}

-(void)cleanupTextures {
    if (_cameraTextureY != nil) {
        CVBufferRelease(_cameraTextureY);
        _cameraTextureY = nil;
    }
    
    if (_cameraTextureCbCr != nil) {
        CVBufferRelease(_cameraTextureCbCr);
        _cameraTextureCbCr = nil;
    }
}

-(void)dealloc {
  [self cleanupTextures];

  if (textureCache != nil) {
      CVMetalTextureCacheFlush(textureCache, 0);
      CFRelease(textureCache);
      textureCache = nil;
  }

  [planeLock release];
  
  [super dealloc];
}

- (void)mtkView:(MTKView *)__unused view drawableSizeWillChange:(CGSize)size {
    _viewportSize.width = size.width;
    _viewportSize.height = size.height;
}

- (void)drawInMTKView:(MTKView *)__unused view {
}

- (CGSize)viewSize {
    return _viewportSize;
}

@end
namespace xr {
    namespace {
        const char* shaderSource = R"(
            #include <metal_stdlib>
            #include <simd/simd.h>

            using namespace metal;

            #include <simd/simd.h>

            typedef struct
            {
                vector_float2 position;
                vector_float2 uv;
                vector_float2 cameraUV;
            } XRVertex;

            typedef struct
            {
                float4 position [[position]];
                float2 uv;
                float2 cameraUV;
            } RasterizerData;

            vertex RasterizerData
            vertexShader(uint vertexID [[vertex_id]],
                         constant XRVertex *vertices [[buffer(0)]])
            {
                RasterizerData out;
                out.position = vector_float4(vertices[vertexID].position.xy, 0.0, 1.0);
                out.uv = vertices[vertexID].uv;
                out.cameraUV = vertices[vertexID].cameraUV;
                return out;
            }

            fragment float4 fragmentShader(RasterizerData in [[stage_in]],
                texture2d<float, access::sample> babylonTexture [[ texture(0) ]],
                texture2d<float, access::sample> cameraTextureY [[ texture(1) ]],
                texture2d<float, access::sample> cameraTextureCbCr [[ texture(2) ]])
            {
                constexpr sampler linearSampler(mip_filter::linear, mag_filter::linear, min_filter::linear);

                const float4 babylonSample = babylonTexture.sample(linearSampler, in.uv);
                if (is_null_texture(cameraTextureY) || is_null_texture(cameraTextureCbCr))
                {
                    return babylonSample;
                }
    
                const float4 cameraSampleY = cameraTextureY.sample(linearSampler, in.cameraUV);
                const float4 cameraSampleCbCr = cameraTextureCbCr.sample(linearSampler, in.cameraUV);

                const float4x4 ycbcrToRGBTransform = float4x4(
                    float4(+1.0000f, +1.0000f, +1.0000f, +0.0000f),
                    float4(+0.0000f, -0.3441f, +1.7720f, +0.0000f),
                    float4(+1.4020f, -0.7141f, +0.0000f, +0.0000f),
                    float4(-0.7010f, +0.5291f, -0.8860f, +1.0000f)
                );

                float4 ycbcr = float4(cameraSampleY.r, cameraSampleCbCr.rg, 1.0);
                float4 cameraSample = ycbcrToRGBTransform * ycbcr;
                cameraSample.a = 1.0;

                const float4 mixed = mix(cameraSample, babylonSample, babylonSample.a);

                return mixed;
            }
        )";

        id<MTLLibrary> CompileShader(id<MTLDevice> metalDevice, const char* source) {
            NSError* error;
            id<MTLLibrary> lib = [metalDevice newLibraryWithSource:@(source) options:nil error:&error];
            if(nil != error) {
                throw std::runtime_error{[error.localizedDescription cStringUsingEncoding:NSASCIIStringEncoding]};
            }
            return lib;
        }
    }
    
    struct System::Impl {
    public:
        Impl(const std::string&) {}

        bool IsInitialized() const {
            return true;
        }

        bool TryInitialize() {
            return true;
        }
    };

    struct System::Session::Impl {
    public:
        const System::Impl& SystemImpl;
        std::vector<Frame::View> ActiveFrameViews{ {} };
        std::vector<Frame::InputSource> InputSources;
        std::vector<Frame::Plane> Planes{};
        std::vector<FeaturePoint> FeaturePointCloud{};
        ARFrame* currentFrame{};
        float DepthNearZ{ DEFAULT_DEPTH_NEAR_Z };
        float DepthFarZ{ DEFAULT_DEPTH_FAR_Z };
        
        Impl(System::Impl& systemImpl, void* graphicsContext, std::function<void*()> windowProvider)
            : SystemImpl{ systemImpl }
            , getMainView{ [windowProvider{ std::move(windowProvider) }] { return reinterpret_cast<UIView*>(windowProvider()); } }
            , metalDevice{ id<MTLDevice>(graphicsContext) } {
            UIView* mainView = getMainView();

            // Create the XR ViewÂ to stay within the safe area of the main view.
            dispatch_sync(dispatch_get_main_queue(), ^{
                xrView = [[MTKView alloc] initWithFrame:mainView.bounds device:metalDevice];
                [mainView addSubview:xrView];
                xrView.userInteractionEnabled = false;
                xrView.colorPixelFormat = MTLPixelFormatBGRA8Unorm;
                xrView.depthStencilPixelFormat = MTLPixelFormatDepth32Float_Stencil8;
                xrView.autoresizingMask = UIViewAutoresizingFlexibleWidth | UIViewAutoresizingFlexibleHeight;
// NOTE: There is an incorrect warning about CAMetalLayer specifically when compiling for the simulator.
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wpartial-availability"
                metalLayer = (CAMetalLayer *)xrView.layer;
#pragma clang diagnostic pop
                metalLayer.device = metalDevice;
                auto scale = UIScreen.mainScreen.scale;
                viewportSize.x = mainView.bounds.size.width * scale;
                viewportSize.y = mainView.bounds.size.height * scale;
            });

            // Create the ARSession enable plane detection, and disable lighting estimation.
            session = [ARSession new];
            auto configuration = [ARWorldTrackingConfiguration new];
            configuration.planeDetection = ARPlaneDetectionHorizontal | ARPlaneDetectionVertical;
            configuration.lightEstimationEnabled = false;
            configuration.worldAlignment = ARWorldAlignmentGravity;

            sessionDelegate = [[SessionDelegate new]init:&ActiveFrameViews metalContext:metalDevice viewportSize:CGSizeMake(viewportSize.x, viewportSize.y)];
            session.delegate = sessionDelegate;
            xrView.delegate = sessionDelegate;
                
            [session runWithConfiguration:configuration];
                
            [configuration release];

            id<MTLLibrary> lib = CompileShader(metalDevice, shaderSource);
            id<MTLFunction> vertexFunction = [lib newFunctionWithName:@"vertexShader"];
            id<MTLFunction> fragmentFunction = [lib newFunctionWithName:@"fragmentShader"];

            // Configure a pipeline descriptor that is used to create a pipeline state.
            MTLRenderPipelineDescriptor *pipelineStateDescriptor = [[MTLRenderPipelineDescriptor alloc] init];
            pipelineStateDescriptor.label = @"XR Pipeline";
            pipelineStateDescriptor.vertexFunction = vertexFunction;
            pipelineStateDescriptor.fragmentFunction = fragmentFunction;
            pipelineStateDescriptor.colorAttachments[0].pixelFormat = metalLayer.pixelFormat;

            // build pipeline
            NSError* error;
            pipelineState = [metalDevice newRenderPipelineStateWithDescriptor:pipelineStateDescriptor error:&error];
            if (!pipelineState) {
                NSLog(@"Failed to create pipeline state: %@", error);
            }
            
            [pipelineStateDescriptor release];
            commandQueue = [metalDevice newCommandQueue];
        }

        ~Impl() {
            if (ActiveFrameViews[0].ColorTexturePointer != nil) {
                id<MTLTexture> oldColorTexture = reinterpret_cast<id<MTLTexture>>(ActiveFrameViews[0].ColorTexturePointer);
                [oldColorTexture setPurgeableState:MTLPurgeableStateEmpty];
                [oldColorTexture release];
                ActiveFrameViews[0].ColorTexturePointer = nil;
            }
            
            if (ActiveFrameViews[0].DepthTexturePointer != nil) {
                id<MTLTexture> oldDepthTexture = reinterpret_cast<id<MTLTexture>>(ActiveFrameViews[0].DepthTexturePointer);
                [oldDepthTexture setPurgeableState:MTLPurgeableStateEmpty];
                [oldDepthTexture release];
                ActiveFrameViews[0].DepthTexturePointer = nil;
            }

            Planes.clear();
            CleanupAnchor(nil);
            [sessionDelegate release];
            [session pause];
            [session release];
            [pipelineState release];
            [xrView releaseDrawables];
            dispatch_sync(dispatch_get_main_queue(), ^{
                [xrView removeFromSuperview]; });
            [xrView release];
            xrView = nil;
        }
        
        /**
         After the ARSession starts, it takes a little time before AR frames become available. This function just makes it easy to roll this into CreateAsync.
         */
        arcana::task<void, std::exception_ptr> WhenReady() {
            __block arcana::task_completion_source<void, std::exception_ptr> tcs;
            CFRunLoopRef mainRunLoop = CFRunLoopGetMain();
            const auto intervalInSeconds = 0.033;
            CFRunLoopTimerRef timer = CFRunLoopTimerCreateWithHandler(kCFAllocatorDefault, CFAbsoluteTimeGetCurrent(), intervalInSeconds, 0, 0, ^(CFRunLoopTimerRef timer){
                if ([session currentFrame] != nil) {
                    CFRunLoopRemoveTimer(mainRunLoop, timer, kCFRunLoopCommonModes);
                    CFRelease(timer);
                    tcs.complete();
                }
            });
            CFRunLoopAddTimer(mainRunLoop, timer, kCFRunLoopCommonModes);
            return tcs.as_task();
        }

        std::unique_ptr<System::Session::Frame> GetNextFrame(bool& shouldEndSession, bool& shouldRestartSession, std::function<void(void* texturePointer)> deletedTextureCallback) {
            shouldEndSession = sessionEnded;
            shouldRestartSession = false;

            // We may or may not be under the scope of an autoreleasepool already, so to guard against both cases grab the
            // current frame inside a locally scoped autoreleasepool and manually retain the frame without marking for autorelease.
            // DEVNOTE: We should change the contract to always run the work queue tick as part of an autoreleasepool so that it is the same for all environments see:
            // https://github.com/BabylonJS/BabylonNative/issues/527
            @autoreleasepool {
                currentFrame = session.currentFrame;
                [currentFrame retain];
            }
            
            dispatch_sync(dispatch_get_main_queue(), ^{
                // Check whether the main view has changed, and if so, reparent the xr view.
                UIView* currentSuperview = [xrView superview];
                UIView* desiredSuperview = getMainView();
                if (currentSuperview != desiredSuperview) {
                    [xrView removeFromSuperview];
                    [xrView setFrame:desiredSuperview.bounds];
                    [desiredSuperview addSubview:xrView];
                }
                
                [sessionDelegate session:session didUpdateFrameInternal:currentFrame];
            });

            auto viewSize = [sessionDelegate viewSize];
            viewportSize.x = viewSize.width;
            viewportSize.y = viewSize.height;
            uint32_t width = viewportSize.x;
            uint32_t height = viewportSize.y;
            
            if (ActiveFrameViews[0].ColorTextureSize.Width != width || ActiveFrameViews[0].ColorTextureSize.Height != height) {
                // Color texture
                {
                    if (ActiveFrameViews[0].ColorTexturePointer != nil) {
                        deletedTextureCallback(ActiveFrameViews[0].ColorTexturePointer);
                        id<MTLTexture> oldColorTexture = reinterpret_cast<id<MTLTexture>>(ActiveFrameViews[0].ColorTexturePointer);
                        [oldColorTexture setPurgeableState:MTLPurgeableStateEmpty];
                        [oldColorTexture release];
                        ActiveFrameViews[0].ColorTexturePointer = nil;
                    }

                    MTLTextureDescriptor *textureDescriptor = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat:MTLPixelFormatBGRA8Unorm width:width height:height mipmapped:NO];
                    textureDescriptor.usage = MTLTextureUsageRenderTarget | MTLTextureUsageShaderRead;
                    id<MTLTexture> texture = [metalDevice newTextureWithDescriptor:textureDescriptor];
                    [texture retain];

                    ActiveFrameViews[0].ColorTexturePointer = reinterpret_cast<void *>(texture);
                    ActiveFrameViews[0].ColorTextureFormat = TextureFormat::RGBA8_SRGB;
                    ActiveFrameViews[0].ColorTextureSize = {width, height};
                }

                // Allocate and store the depth texture
                {
                    if (ActiveFrameViews[0].DepthTexturePointer != nil) {
                        id<MTLTexture> oldDepthTexture = reinterpret_cast<id<MTLTexture>>(ActiveFrameViews[0].DepthTexturePointer);
                        [oldDepthTexture setPurgeableState:MTLPurgeableStateEmpty];
                        [oldDepthTexture release];
                        ActiveFrameViews[0].DepthTexturePointer = nil;
                    }

                    MTLTextureDescriptor *textureDescriptor = [MTLTextureDescriptor texture2DDescriptorWithPixelFormat:MTLPixelFormatDepth32Float_Stencil8 width:width height:height mipmapped:NO];
                    textureDescriptor.storageMode = MTLStorageModePrivate;
                    textureDescriptor.usage = MTLTextureUsageRenderTarget;
                    id<MTLTexture> texture = [metalDevice newTextureWithDescriptor:textureDescriptor];
                    [texture retain];

                    ActiveFrameViews[0].DepthTexturePointer = reinterpret_cast<void *>(texture);
                    ActiveFrameViews[0].DepthTextureFormat = TextureFormat::D24S8;
                    ActiveFrameViews[0].DepthTextureSize = {width, height};
                }
            }
            else {
                @autoreleasepool {
                    // Clear the color and depth texture before handing it off to Babylon
                    id<MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];
                    commandBuffer.label = @"BabylonTextureClearBuffer";
                    MTLRenderPassDescriptor *renderPassDescriptor = [MTLRenderPassDescriptor renderPassDescriptor];
                    if(renderPassDescriptor != nil) {
                        // Set up the clear for the color texture.
                        renderPassDescriptor.colorAttachments[0].texture = reinterpret_cast<id<MTLTexture>>(ActiveFrameViews[0].ColorTexturePointer);
                        renderPassDescriptor.colorAttachments[0].loadAction = MTLLoadActionClear;
                        renderPassDescriptor.colorAttachments[0].clearColor = MTLClearColorMake(0.0,0.0,0.0,0.0);
                        
                        // Set up the clear for the depth texture.
                        renderPassDescriptor.depthAttachment.texture = reinterpret_cast<id<MTLTexture>>(ActiveFrameViews[0].DepthTexturePointer);
                        renderPassDescriptor.depthAttachment.loadAction = MTLLoadActionClear;
                        renderPassDescriptor.depthAttachment.clearDepth = 1.0f;
                        
                        // Create and end the render encoder.
                        id<MTLRenderCommandEncoder> renderEncoder = [commandBuffer renderCommandEncoderWithDescriptor:renderPassDescriptor];
                        renderEncoder.label = @"BabylonTextureClearEncoder";
                        [renderEncoder endEncoding];
                    }
                    
                    // Finalize rendering here & push the command buffer to the GPU.
                    [commandBuffer commit];
                    [commandBuffer waitUntilCompleted];
                }
            }
            
            return std::make_unique<Frame>(*this);
        }

        void RequestEndSession() {
            // Note the end session has been requested, and respond to the request in the next call to GetNextFrame
            sessionEnded = true;
        }

        Size GetWidthAndHeightForViewIndex(size_t) const {
            // Return a valid (non-zero) size, but otherwise it doesn't matter as the render texture created from this isn't currently used
            return {1,1};
        }
        
        void DrawFrame() {
            @autoreleasepool {
                // Create a new command buffer for each render pass to the current drawable.
                id<MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];
                commandBuffer.label = @"XRDisplayCommandBuffer";
                
                id<CAMetalDrawable> drawable = [metalLayer nextDrawable];
                MTLRenderPassDescriptor *renderPassDescriptor = [MTLRenderPassDescriptor renderPassDescriptor];
                
                id<MTLTexture> cameraTextureY = nil;
                id<MTLTexture> cameraTextureCbCr = nil;
                @synchronized(sessionDelegate) {
                    cameraTextureY = [sessionDelegate GetCameraTextureY];
                    cameraTextureCbCr = [sessionDelegate GetCameraTextureCbCr];
                }
                
                [commandBuffer addCompletedHandler:^(id<MTLCommandBuffer>) {
                    if (cameraTextureY != nil) {
                        [cameraTextureY setPurgeableState:MTLPurgeableStateEmpty];
                    }

                    if (cameraTextureCbCr != nil) {
                        [cameraTextureCbCr setPurgeableState:MTLPurgeableStateEmpty];
                    }
                }];
                
                @try {
                    if(renderPassDescriptor != nil) {
                        renderPassDescriptor.colorAttachments[0].texture = drawable.texture;
                        renderPassDescriptor.colorAttachments[0].loadAction = MTLLoadActionClear;
                        renderPassDescriptor.colorAttachments[0].clearColor = MTLClearColorMake(0.0,0.0,0.0,1.0);
                        
                        // Create a render command encoder.
                        id<MTLRenderCommandEncoder> renderEncoder = [commandBuffer renderCommandEncoderWithDescriptor:renderPassDescriptor];
                        renderEncoder.label = @"XRDisplayEncoder";

                        // Set the region of the drawable to draw into.
                        [renderEncoder setViewport:(MTLViewport){0.0, 0.0, static_cast<double>(viewportSize.x), static_cast<double>(viewportSize.y), 0.0, 1.0 }];
                        
                        [renderEncoder setRenderPipelineState:pipelineState];

                        // Pass in the parameter data.
                        [renderEncoder setVertexBytes:vertices length:sizeof(vertices) atIndex:0];

                        [renderEncoder setFragmentTexture:id<MTLTexture>(ActiveFrameViews[0].ColorTexturePointer) atIndex:0];
                        [renderEncoder setFragmentTexture:cameraTextureY atIndex:1];
                        [renderEncoder setFragmentTexture:cameraTextureCbCr atIndex:2];

                        // Draw the triangles.
                        [renderEncoder drawPrimitives:MTLPrimitiveTypeTriangleStrip vertexStart:0 vertexCount:4];

                        [renderEncoder endEncoding];

                        // Schedule a present once the framebuffer is complete using the current drawable.
                        [commandBuffer presentDrawable:drawable];
                    }

                    // Finalize rendering here & push the command buffer to the GPU.
                    [commandBuffer commit];
                    
                    if (currentFrame != nil) {
                        [currentFrame release];
                        currentFrame = nil;
                    }
                }
                @catch (NSException* exception) {
                    if (cameraTextureY != nil) {
                        [cameraTextureY setPurgeableState:MTLPurgeableStateEmpty];
                    }
                    
                    if (cameraTextureCbCr != nil) {
                        [cameraTextureCbCr setPurgeableState:MTLPurgeableStateEmpty];
                    }
                    
                    @throw;
                }
            }
        }

        void GetHitTestResults(std::vector<HitResult>& filteredResults, xr::Ray offsetRay, xr::HitTestTrackableType trackableTypes) const {
            @autoreleasepool {
                if (currentFrame != nil && currentFrame.camera != nil && [currentFrame.camera trackingState] == ARTrackingStateNormal) {
                    if (@available(iOS 13.0, *)) {
                        GetHitTestResultsForiOS13(filteredResults, offsetRay, trackableTypes);
                    } else {
                        GetHitTestResultsLegacy(filteredResults, trackableTypes);
                    }
                }
            }
        }

        /**
         Create an ARKit anchor for the given pose.
         */
        xr::Anchor CreateAnchor(Pose pose) {
            // Pull out the pose into a float 4x4 transform that is usable by ARKit.
            auto poseTransform = PoseToTransform(pose);
            
            // Create the anchor and add it to the ARKit session.
            auto anchor = [[ARAnchor alloc] initWithTransform:poseTransform];
            [session addAnchor:anchor];
            nativeAnchors.push_back(anchor);
            return { pose, reinterpret_cast<NativeAnchorPtr>(anchor) };
        }
        
        /**
         For a given anchor update the current pose, and determine if it is still valid.
         */
        void UpdateAnchor(xr::Anchor& anchor) {
            // First check if the anchor still exists, if not then mark the anchor as no longer valid.
            auto arAnchor = reinterpret_cast<ARAnchor*>(anchor.NativeAnchor);
            if (arAnchor == nil) {
                anchor.IsValid = false;
                return;
            }
            
            // Then update the anchor's pose based on its transform.
            anchor.Pose = TransformToPose(arAnchor.transform);
        }
        
        /**
         Deletes the ArKit anchor associated with this XR anchor if it still exists.
         */
        void DeleteAnchor(xr::Anchor& anchor) {
            // If this anchor has not already been deleted, then remove it from the current AR session,
            // and clean up its state in memory.
            if (anchor.NativeAnchor != nil) {
                auto arAnchor = reinterpret_cast<ARAnchor*>(anchor.NativeAnchor);
                anchor.NativeAnchor = nil;

                CleanupAnchor(arAnchor);
            }
        }
        
        /**
         Updates existing planes in place, gets the list of updated/created plane IDs, and removed plane IDs.
         */
        void UpdatePlanes(std::vector<Frame::Plane::Identifier>& updatedPlanes, std::vector<Frame::Plane::Identifier>& deletedPlanes) {
            if (!planeDetectionEnabled)
            {
                return;
            }
            
            [sessionDelegate LockPlanes];
            @try {
                // First lets go and update all planes that have been updated since the last frame.
                auto updatedARKitPlanes = [sessionDelegate GetUpdatedPlanes];
                for (ARPlaneAnchor* updatedPlane : *updatedARKitPlanes) {
                    // Dynamically allocate the polygon array, and fill it in.
                    auto geometry = updatedPlane.geometry;
                    auto polygonSize = geometry.boundaryVertexCount;
                    
                    planePolygonBuffer.clear();
                    planePolygonBuffer.resize(polygonSize * 3);
                    for (NSUInteger i = 0; i < polygonSize; i++) {
                        NSUInteger polygonIndex =  i * 3;
                        planePolygonBuffer[polygonIndex] = geometry.boundaryVertices[i].x;
                        planePolygonBuffer[polygonIndex + 1] = geometry.boundaryVertices[i].y;
                        planePolygonBuffer[polygonIndex + 2] = geometry.boundaryVertices[i].z;
                    }

                    // Update the existing plane if it exists, otherwise create a new plane, and add it to our list of planes.
                    auto planeIterator = planeMap.find(updatedPlane.identifier);
                    if (planeIterator != planeMap.end()) {
                        UpdatePlane(updatedPlanes, GetPlaneByID(planeIterator->second), updatedPlane, planePolygonBuffer, polygonSize);
                    } else {
                        // This is a new plane, create it and initialize its values.
                        Planes.emplace_back();
                        auto& plane = Planes.back();
                        [updatedPlane.identifier retain];
                        planeMap.insert({updatedPlane.identifier, plane.ID});
                        
                        // Fill in the polygon and center pose.
                        UpdatePlane(updatedPlanes, plane, updatedPlane, planePolygonBuffer, polygonSize);
                    }
                    
                    [updatedPlane release];
                }
                
                // Clear the list of updated planes to start building up for the next frame update.
                updatedARKitPlanes->clear();
                
                // Now loop over all deleted planes find them in the existing planes map and if the entry exists add it to the list of removed planes.
                auto removedARKitPlanes = [sessionDelegate GetDeletedPlanes];
                for (ARPlaneAnchor* removedPlane: *removedARKitPlanes) {
                    // Find the plane in the set of existing planes.
                    auto planeIterator = planeMap.find(removedPlane.identifier);
                    if (planeIterator != planeMap.end()) {
                        // Release the held ref to the native plane ID and clean up its polygon as it is no longer needed.
                        auto [nativePlaneID, planeID] = *planeIterator;
                        deletedPlanes.push_back(planeID);

                        auto& plane = GetPlaneByID(planeID);
                        plane.Polygon.clear();
                        plane.PolygonSize = 0;
                        planeMap.erase(planeIterator);
                        [nativePlaneID release];
                    }
                    
                    [removedPlane release];
                }
                
                // Clear the list of removed frames to start building up for the next plane update.
                removedARKitPlanes->clear();
            } @finally {
                [sessionDelegate UnlockPlanes];
            }
        }

        Frame::Plane& GetPlaneByID(Frame::Plane::Identifier planeID)
        {
            // Loop over the plane vector and find the correct plane.
            for (Frame::Plane& plane : Planes)
            {
                if (plane.ID == planeID)
                {
                    return plane;
                }
            }

            throw std::runtime_error{"Tried to get non-existent plane."};
        }

        /**
         Deallocates the native ARKit anchor object, and removes it from the anchor list.
         */
        void CleanupAnchor(ARAnchor* arAnchor) {
            // Iterate over the list of anchors if arAnchor is nil then clean up all anchors
            // otherwise clean up only the target anchor and return.
            auto anchorIter = nativeAnchors.begin();
            while (anchorIter != nativeAnchors.end()) {
                if (arAnchor == nil || arAnchor == *anchorIter) {
                    [session removeAnchor:*anchorIter];
                    [*anchorIter release];
                    anchorIter = nativeAnchors.erase(anchorIter);

                    if (arAnchor != nil) {
                        return;
                    }
                }
                else {
                    anchorIter++;
                }
            }
        }
        
        void SetPlaneDetectionEnabled(bool enabled)
        {
            planeDetectionEnabled = enabled;
            [sessionDelegate SetPlaneDetectionEnabled:enabled];
        }
        
        bool IsTracking() const {
            // There are three different tracking states as defined in ARKit: https://developer.apple.com/documentation/arkit/artrackingstate
            // From my testing even while obscuring the camera for a long duration the state still registers as ARTrackingStateLimited
            // rather than ARTrackingStateNotAvailable. For that reason the only state that should be considered to be trully tracking is
            // ARTrackingStateNormal.
            return currentFrame.camera.trackingState == ARTrackingState::ARTrackingStateNormal;
        }

    private:
        ARSession* session{};
        std::function<UIView*()> getMainView{};
        MTKView* xrView{};
        bool sessionEnded{ false };
        id<MTLDevice> metalDevice{};
// NOTE: There is an incorrect warning about CAMetalLayer specifically when compiling for the simulator.
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wpartial-availability"
        CAMetalLayer* metalLayer{};
#pragma clang diagnostic pop
        SessionDelegate* sessionDelegate{};
        id<MTLRenderPipelineState> pipelineState{};
        vector_uint2 viewportSize{};
        id<MTLCommandQueue> commandQueue;
        std::vector<ARAnchor*> nativeAnchors{};
        std::vector<float> planePolygonBuffer{};
        std::unordered_map<NSUUID*, Frame::Plane::Identifier> planeMap{};
        bool planeDetectionEnabled{ false };
        
        /*
         Helper function to translate a world transform into a hit test result.
         */
        HitResult transformToHitResult(simd_float4x4 transform) const {
            auto orientation = simd_quaternion(transform);
            HitResult hitResult{};
            hitResult.Pose.Orientation = {
                orientation.vector.x,
                orientation.vector.y,
                orientation.vector.z,
                orientation.vector.w
            };
            hitResult.Pose.Position = {
                transform.columns[3][0],
                transform.columns[3][1],
                transform.columns[3][2]
            };
            
            return hitResult;
        }
        
        void UpdatePlane(std::vector<Frame::Plane::Identifier>& updatedPlanes, Frame::Plane& plane, ARPlaneAnchor* planeAnchor, std::vector<float>& newPolygon, size_t polygonSize) {
            Pose newCenter = TransformToPose(planeAnchor.transform);

            // If the plane was not actually updated, free the polygon buffer, and return.
            if (!CheckIfPlaneWasUpdated(plane, newPolygon, newCenter)) {
                return;
            }
            
            // Update the center of the plane
            plane.Center = newCenter;

            // Store the polygon.
            plane.Polygon.swap(newPolygon);
            plane.PolygonSize = polygonSize;
            plane.PolygonFormat = PolygonFormat::XYZ;
            updatedPlanes.push_back(plane.ID);
        }
                
        // For iOS 13.0 and up make use of the ARRaycastQuery protocol for raycasting against all target trackable types.
        API_AVAILABLE(ios(13.0))
        void GetHitTestResultsForiOS13(std::vector<HitResult>& filteredResults, xr::Ray offsetRay, xr::HitTestTrackableType trackableTypes) const{
            // Push the camera origin into a simd_float3.
            auto cameraOrigin = simd_make_float3(
                                                 ActiveFrameViews[0].Space.Pose.Position.X,
                                                 ActiveFrameViews[0].Space.Pose.Position.Y,
                                                 ActiveFrameViews[0].Space.Pose.Position.Z);
            
            // Push the camera direction into a simd_quaternion.
            auto cameraDirection = simd_quaternion(
                                                   ActiveFrameViews[0].Space.Pose.Orientation.X,
                                                   ActiveFrameViews[0].Space.Pose.Orientation.Y,
                                                   ActiveFrameViews[0].Space.Pose.Orientation.Z,
                                                   ActiveFrameViews[0].Space.Pose.Orientation.W);
            
            // Load the offset ray and direction into simd equivalents.
            auto offsetOrigin = simd_make_float3(offsetRay.Origin.X, offsetRay.Origin.Y, offsetRay.Origin.Z);
            auto offsetDirection = simd_make_float3(offsetRay.Direction.X, offsetRay.Direction.Y, offsetRay.Direction.Z);
            auto rayOrigin = cameraOrigin + offsetOrigin;
            auto rayDirection = simd_act(cameraDirection, offsetDirection);
            
            // Check which types we are meant to raycast against and perform their respective queries.
            if ((trackableTypes & xr::HitTestTrackableType::MESH) != xr::HitTestTrackableType::NONE) {
                PerformRaycastQueryAgainstTarget(filteredResults, ARRaycastTargetExistingPlaneGeometry, rayOrigin, rayDirection);
            }
            
            if ((trackableTypes & xr::HitTestTrackableType::POINT) != xr::HitTestTrackableType::NONE) {
                PerformRaycastQueryAgainstTarget(filteredResults, ARRaycastTargetEstimatedPlane, rayOrigin, rayDirection);
            }
            
            if ((trackableTypes & xr::HitTestTrackableType::PLANE) != xr::HitTestTrackableType::NONE) {
                PerformRaycastQueryAgainstTarget(filteredResults, ARRaycastTargetExistingPlaneInfinite, rayOrigin, rayDirection);
            }
        }
        
        API_AVAILABLE(ios(13.0))
        void PerformRaycastQueryAgainstTarget(std::vector<HitResult>& filteredResults, ARRaycastTarget targetType, simd_float3 origin, simd_float3 direction) const {
            auto raycastQuery = [[ARRaycastQuery alloc]
                                 initWithOrigin:origin
                                 direction:direction
                                 allowingTarget:targetType
                                 alignment:ARRaycastTargetAlignmentAny];
            
            // Perform the actual raycast.
            auto rayCastResults = [session raycast:raycastQuery];
            [raycastQuery release];
            
            // Process the results and push them into the results list.
            for (ARRaycastResult* result in rayCastResults) {
                filteredResults.push_back(transformToHitResult(result.worldTransform));
            }
        }
        
        // On iOS versions prior to 13, fall back to doing a raycast from a screen point, for now don't support translating the offset ray.
        void GetHitTestResultsLegacy(std::vector<HitResult>& filteredResults, xr::HitTestTrackableType trackableTypes) const {
            // First set the type filter based on the requested trackable types.
            ARHitTestResultType typeFilter = 0;
            if ((trackableTypes & xr::HitTestTrackableType::POINT) != xr::HitTestTrackableType::NONE) {
                typeFilter |= ARHitTestResultTypeFeaturePoint;
            }
            
            if ((trackableTypes & xr::HitTestTrackableType::PLANE) != xr::HitTestTrackableType::NONE) {
                typeFilter |= ARHitTestResultTypeExistingPlane;
            }

            if ((trackableTypes & xr::HitTestTrackableType::POINT) != xr::HitTestTrackableType::NONE) {
                if (@available(iOS 11.3, *)) {
                    typeFilter |= ARHitTestResultTypeExistingPlaneUsingGeometry;
                } else {
                    typeFilter |= ARHitTestResultTypeExistingPlaneUsingExtent;
                }
            }

            // Now perform the actual hit test and process the results
            auto hitTestResults = [currentFrame hitTest:CGPointMake(.5, .5) types:(typeFilter)];
            for (ARHitTestResult* result in hitTestResults) {
                filteredResults.push_back(transformToHitResult(result.worldTransform));
            }
        }
    };

    struct System::Session::Frame::Impl {
    public:
        Impl(Session::Impl& sessionImpl)
            : sessionImpl{sessionImpl} { }

        Session::Impl& sessionImpl;
    };

    System::Session::Frame::Frame(Session::Impl& sessionImpl)
        : Views{ sessionImpl.ActiveFrameViews }
        , InputSources{ sessionImpl.InputSources}
        , Planes{ sessionImpl.Planes }
        , FeaturePointCloud{ sessionImpl.FeaturePointCloud } // NYI
        , UpdatedPlanes{}
        , RemovedPlanes{}
        , IsTracking{sessionImpl.IsTracking()}
        , m_impl{ std::make_unique<System::Session::Frame::Impl>(sessionImpl) } {
        Views[0].DepthNearZ = sessionImpl.DepthNearZ;
        Views[0].DepthFarZ = sessionImpl.DepthFarZ;
        m_impl->sessionImpl.UpdatePlanes(UpdatedPlanes, RemovedPlanes);
    }

    System::Session::Frame::~Frame() {
        m_impl->sessionImpl.DrawFrame();
    }

    void System::Session::Frame::GetHitTestResults(std::vector<HitResult>& filteredResults, xr::Ray offsetRay, xr::HitTestTrackableType trackableTypes) const {
        m_impl->sessionImpl.GetHitTestResults(filteredResults, offsetRay, trackableTypes);
    }

    Anchor System::Session::Frame::CreateAnchor(Pose pose, NativeTrackablePtr) const {
        return m_impl->sessionImpl.CreateAnchor(pose);
    }

    void System::Session::Frame::UpdateAnchor(xr::Anchor& anchor) const {
        m_impl->sessionImpl.UpdateAnchor(anchor);
    }

    void System::Session::Frame::DeleteAnchor(xr::Anchor& anchor) const {
        m_impl->sessionImpl.DeleteAnchor(anchor);
    }

    System::Session::Frame::Plane& System::Session::Frame::GetPlaneByID(System::Session::Frame::Plane::Identifier planeID) const {
        return m_impl->sessionImpl.GetPlaneByID(planeID);
    }

    System::System(const char* appName)
        : m_impl{ std::make_unique<System::Impl>(appName) } {}

    System::~System() {}

    bool System::IsInitialized() const {
        return m_impl->IsInitialized();
    }

    bool System::TryInitialize() {
        return m_impl->TryInitialize();
    }

    arcana::task<bool, std::exception_ptr> System::IsSessionSupportedAsync(SessionType sessionType) {
        // Only IMMERSIVE_AR is supported for now.
        return arcana::task_from_result<std::exception_ptr>(sessionType == SessionType::IMMERSIVE_AR && ARWorldTrackingConfiguration.isSupported);
    }

    arcana::task<std::shared_ptr<System::Session>, std::exception_ptr> System::Session::CreateAsync(System& system, void* graphicsDevice, std::function<void*()> windowProvider) {
        auto session = std::make_shared<System::Session>(system, graphicsDevice, std::move(windowProvider));
        return session->m_impl->WhenReady().then(arcana::inline_scheduler, arcana::cancellation::none(), [session] {
            return session;
        });
    }

    System::Session::Session(System& system, void* graphicsDevice, std::function<void*()> windowProvider)
        : m_impl{ std::make_unique<System::Session::Impl>(*system.m_impl, graphicsDevice, std::move(windowProvider)) } {}

    System::Session::~Session() {
        // Free textures
    }

    std::unique_ptr<System::Session::Frame> System::Session::GetNextFrame(bool& shouldEndSession, bool& shouldRestartSession, std::function<void(void* texturePointer)> deletedTextureCallback) {
        return m_impl->GetNextFrame(shouldEndSession, shouldRestartSession, deletedTextureCallback);
    }

    void System::Session::RequestEndSession() {
        m_impl->RequestEndSession();
    }

    Size System::Session::GetWidthAndHeightForViewIndex(size_t viewIndex) const {
        return m_impl->GetWidthAndHeightForViewIndex(viewIndex);
    }

    void System::Session::SetDepthsNearFar(float depthNear, float depthFar) {
        m_impl->DepthNearZ = depthNear;
        m_impl->DepthFarZ = depthFar;
    }

    void System::Session::SetPlaneDetectionEnabled(bool enabled) const
    {
        m_impl->SetPlaneDetectionEnabled(enabled);
    }

    bool System::Session::TrySetFeaturePointCloudEnabled(bool) const
    {
        // Point cloud system not yet supported.
        return false;
    }
}
